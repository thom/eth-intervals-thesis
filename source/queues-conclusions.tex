%==============================================================================
% queues-conclusions.tex
%==============================================================================

\chapter{Conclusions}
\label{chap:queues-conclusions}

\todo[inline]{Finish chapter ``Conclusions''}

We could not find a huge difference when running the benchmarks using
the alternative queue implementations. The paper ``Enabling
scalability and performance in a large scale CMP environment''
\cite{Saha2007} states that there is no noticeable difference between
the speedup of work-stealing and global shared work queue when using
not more than 8 cores.

To check this hypothesis, we rewrote the intervals scheduler to use a
single shared work deque and use it in a LIFO (stack) manner.

\todo[inline]{Mention the experiment with a single shared work queue}

\section{References}

\subsection{Limits of work-stealing scheduling \cite{Vrba2009}}

The original work-stealing algorithm uses non-blocking algorithms to
implement queue operations \cite{Arora2001}. However, we have decided
to simplify our scheduler implementation by protecting each run queue
with its own lock. We believed that this would not impact scalability
on our machine, because others \cite{Saha2007} have reported that even
a single, centralized queue protected by a single, central lock does
not hurt performance on up to 8 CPUs, which is a decidedly worse
situation for scalability as the number of CPUs grows. Since we use
locks to protect the run queues, and our networks are static, our
implementation does not benefit from the first two advantages of
accessing the run queues at different ends:

The reasons for accessing the run queues at different ends are several
\cite{Frigo1998}: 1) it reduces contention by having stealing threads
operate on the opposite end of the queue than the thread they are
stealing from; 2) it works better for parallelized divide-and-conquer
algorithms which typically generate large chunks of work early, so the
older stolen task is likely to further provide more work to the
stealing thread; 3) stealing a process also migrates its future
workload, which helps to increase locality.

Nevertheless, this helps with increasing locality: since the arrival
of a message unblocks a proces, placing it at the front of the ready
queue increases probability that the required data will remain in the
CPU’s caches.


\section{Related Work}
\label{sec:queues-conclusion-related-work}

\todo[inline]{Finish section ``Related Work''}

Coroutines \cite{Conway1963} and continuations \cite{Reynolds1993} are
two building blocks for manipulating control-flow in a sequential
context. Either would make a useful primitive on which to build the
intervals library and would provide an alternative to rewriting
programs in an event-oriented style.

Futures are annotations for parallel execution which act similarly to
a lazy or deferred execution. Expressions annotated as being safe for
parallel execution are executed in parallel; when the program reaches
a point where the result of the expression is needed, the main thread
blocks until the evaluation of the expression has completed. Futures
were first implemented for MULTILISP \cite{Halstead1985} but have
since been ported to a number of other languages, including Java
\cite{Navabi2008}.  Futures can be seen as a subset of intervals that
lack the extended happens before relationships. Furthermore, most
implementations of futures make no guarantees with respect to
deadlock-freedom or other safety properties.

Jade \cite{Rinard1998} uses programmer-provided specifications to
dynamically parallelize a program. Shared objects were specially
integrated into the type system. Tasks declare those objects that they
affect and how; adherence to these declarations is checked
dynamically. The ability for intervals to be associated with locks
works in a similar fashion, but Jade did not attempt to model happens
before relationships in its task specifications.

Erlang \cite{Erlang2010} embodies a strict share-nothing philosophy,
in which actors with disjoint heaps communicate with messages. The
simplicity of this approach is appealing, but we believe there are
many scenarios where shared memory is an easier and better choice,
given the right tools.

Cilk \cite{Blumofe1995, Frigo1998} and JCilk \cite{Danaher2005} are
supersets of C and Java respectively which add support for
parallelism, primarily in the form of fork-join or barrier style
computations. Cilk pioneered many of the dynamic scheduling and work
stealing techniques used in the intervals implementation itself.

Cilk \cite{Blumofe1995, Frigo1998} and OpenMP 3.0 \cite{OpenMP2008}
both offer lightweight task frameworks where tasks are executed in a
tree structure.  Tasks in these languages are not first-class objects,
however, and they do not support arbitrary dependency graphs. Java's
Fork-Join Framework \cite{Lea2006, Lea2000, Lea2000a} and Intel's
Threading Building Blocks (TBB) \cite{Reinders2007, Contreras2008}
both offer a more flexible alternative, but lack a higher-level
interface to task dependencies. The fork-join framework permits
lightweight tasks to be joined, and TBB allows tasks to delay starting
until an associated counter is decremented to zero.

JSR166 \cite{Lea2004} introduced a number of concurrency-related
utility classes to Java, including futures, thread pools, read-write
locks, and concurrent containers such as maps and queues. Java 7 will
likely contain additional classes \cite{Lea2006}, among them the
fork-join framework that intervals itself is built upon. For C\#, the
Parallel Extensions \cite{Leijen2009} library promises a similar
lightweight task framework.  Neither of these frameworks includes any
mechanism for declarative or explicit happens before relationships;
instead, users use traditional joins to wait for tasks to complete.

The parallel extensions for .NET \cite{Leijen2009} offer a task
library with a similar feeling to intervals. In addition to the usual
join-based task routines, they also permit tasks to have
continuations, which are dependent tasks that execute and are given
the result of the previous task to begin (the equivalent of edges from
the end of one interval to the start of another). This approach is
powerful but does not permit the full range of happens before edges
supported by intervals.

X10 \cite{Charles2005, Saraswat2010} offers a revised threading model
which includes a number of innovative synchronization
constructs. Among them are phasers \cite{Shirako2008, Shirako2010}, a
combination of barriers and signals which can guarantee data-race
freedom. Intervals can be used to construct the same patterns as
phasers with similar guarantees, but also go further by replacing
thread joins and other constructs in the X10 toolset.

X10 \cite{Charles2005, Saraswat2010} introduced a number of innovative
synchronization constructs. The most recent, phasers
\cite{Shirako2008, Shirako2010}, are a combination of barriers and
signals. Threads wishing to synchronize with one another make use of a
shared phaser object.  Threads indicate how they will use a phaser by
placing it into different modes, such as signal-wait-next or
wait-only, that grant different capabilities. A combination of static
and dynamic safety checks ensures that programs cannot be deadlocked
through using a phaser. When synchronizing on a barrier, a special
“single” mode allows a small section of code to be executed by a
single thread before the waiting threads resume. Intervals can be used
to perform the same kinds of synchronizations as phasers and with
similar safety guarantees. However, intervals are a standalone
mechanism that also replaces threads, thread joins, and integrates
locks, all of which are beyond the scope of phasers. On the other
hand, phasers are closer to existing threading primitives and
therefore can be adopted more easily.

OpenMP \cite{OpenMP2008} and the Message Passing Interface (MPI)
\cite{MPI2009} are two higher-level alternatives to threads for
writing parallel programs. Unlike intervals, they are focussed on SIMD
programming, although both can be used more generally.

Apple's Cocoa framework includes a class NSOperation \cite{Apple2008}
that is similar to intervals. Like an Interval, each NSOperation
embodies a particular task, and a user may declaratively specify that
one operation cannot execute until another has finished (the
equivalent of a startAfterEndOf() dependency). Unlike intervals,
however, NSOperations do not permit other kinds of dependencies nor
are they integrated with locks. This means that they cannot easily be
used to describe the patterns in this paper, with the exception of
point to point synchronization.

Intervals in Java align nicely with the Java Memory Model
\cite{Manson2005}: The happens before relation defined by intervals
can be seen as a deterministic subset of the full happens before
relation defined by the memory model, which includes edges due to
constructs like volatile fields or synchronized sections.

The Java Memory Model \cite{Manson2005} defines how parallel threads
in Java interact with shared memory. In addition to defining formally
what it means for a Java program to be correctly synchronized, it
describes the legal behaviors of incorrectly synchronized programs
which include data races. In the Java Memory Model, happens before
relationships potentially result from imperative actions such as
acquiring locks, accessing volatile fields, or joining a thread. This
model can be easily adapted to the explicit happens before
relationships used by intervals. When using our data race analysis,
however, there is no need to define the semantics of incorrectly
synchronized programs, because they cannot occur.


\section{Limitations and Future Work}
\label{sec:queues-conclusion-future-work}

\todo[inline]{Finish section ``Limitations and Future Work''}

\begin{itemize}
\item Non-blocking steal-half work queues \cite{Hendler2002}
\item Let benchmarks run on more than 8 cores
\end{itemize}

\subsection{Dynamic circular work-stealing deque \cite{Chase2005}}

\subsubsection{Shrinking After Growth}

One disadvantage of the algorithm as presented is that it does not
shrink the array as the deque retreats from its maximum. That means
that the memory used by the deque is a constant factor times its
maximum size, which might result in a big waste of memory.

Shrinking the array is no harder than growing the array; it only
requires that the algorithm check against a minimum use fraction of
the current array when performing a \lstinline!take!
operation.\footnote{With the current implementation, this fraction
  must be strictly less than $\frac{1}{2}$, to guarantee that the
  deque elements could fit into the smaller array while leaving one
  array cell unused.} The code for the \lstinline!take!
operation with the possible shrinking operation appears in Figures 5
and 6. As illustrated by the code, Line 31 was modified to call the
\lstinline!perhapsShrink! method just before returning the popped
value. The \lstinline!perhapsShrink! method shrinks the array if the
number of elements in the deque is less than some fraction
$\frac{1}{K}$ of the array size, where $K \ge 3$. We omit the code for
the \lstinline!CircularArray!'s shrink method since it is almost
identical to the code of this class's \lstinline!grow! method.
Finally, note that the \lstinline!perhapsShrink! method is independent
of the \lstinline!take! operation, and therefore can be invoked
by the deque's owner on other occasions (for example after a
\lstinline!put! operation).

\minisec{Shrinking without copying}

The simplest way to shrink back to a smaller array is similar to the
way we grow it: Allocate a new smaller array, and copy the data from
the big array to the smaller one. We can save the allocation time,
however, if whenever we extend an array, we retain a reference from
the bigger array to the smaller one. If each array has a reference to
the smaller array from which it was extended, then the garbage
collector cannot deallocate all the arrays that precede the current
active one, and the algorithm can reuse these arrays when shrinking.

Keeping the references to the smaller arrays not only saves the
allocation time, it can also save some of the copying work: when the
algorithm shrinks back from the big array to its previous smaller
array, only the elements that were modified while the bigger array was
active need to be copied (because the smaller array was not
deallocated and therefore was not modified while the bigger array was
active). This can be accomplished by maintaining a low-water-mark with
each array: an integer that indicates the lowest value of
\lstinline!bottom! in which an element was stored while the array was
active. When a deque shrinks its array, only the elements stored in
indexes greater than or equal to the low water mark of the bigger
array are copied. Also, the smaller array's low water mark is updated
to the minimum of the larger and smaller array's low water mark
values.

Note that the space overhead for referencing all the smaller arrays
when growing is relatively low: if we double the array size every time
we grow the array, the total overhead is less than the size of the
current array.

\minisec{Combining multiple shrinks}

Sometimes it is useful to combine multiple shrink operations, that is
to shrink back not to the previous smaller array, but to one (or more)
preceding it. For example, suppose that we had 5 growing operations:
$a_1 \rightarrow a_2 \rightarrow a_3 \rightarrow a_4 \rightarrow a_5$
(here $a_i$ represents an array, and $a_{i+1}$ is a bigger array than
$a_i$), and that on the next \lstinline!take! operation we find
out that almost all the deque elements were stolen, and that the
number of elements left is less than some fraction of the size of
$a_1$. In such a case it makes more sense to shrink from $a_5$
directly to $a_1$, without going through all the intermediate
arrays. Extra caution should be taken, however, when choosing which
entries to copy using the low water mark: when copying from $a_5$ to
$a_1$ , the low water mark is the minimum of the low water marks of
$a_5$ and all the intermediate arrays (that is $a_4$, $a_3$ and
$a_2$).

\subsubsection{Working With a Shared Pool of Buffers}

The deque algorithm presented depends upon a garbage collector to
reclaim the unused buffers. For work-stealing algorithms, where each
process has its own deque and the maximum amount of memory needed by
all deques together can usually be bounded, it is often more suitable
to use the shared pool model.

With the shared pool model, the extra available buffers for all the
deques are kept in a shared pool. Whenever the deque's owner needs a
bigger array, it allocates one (of the appropriate size) from the
pool, and whenever it shrinks to a smaller array and does not need the
bigger array anymore, it can return it to the pool. There are two main
advantages for the shared pool model: First, it is much less expensive
to reclaim and allocate buffers from the shared pool than to allocate
them from the heap and use the garbage collec- tor for
reclamation. Second, as described in this section, by assuming that
the reclaimed buffers are not returned to a global use by the
operating system, the deque's owner can reclaim a buffer while there
may be still some thieves refer- encing it (something that the garage
collector will not do), which leads to a better use of the allocated
space.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
