%==============================================================================
% queues-conclusion.tex
%==============================================================================

\chapter{Conclusion}
\label{chap:queues-conclusion}

\todo[inline]{Finish chapter ``Conclusion''}

We could not find a huge difference when running the benchmarks using
the alternative queue implementations. The paper ``Enabling
scalability and performance in a large scale CMP environment''
\cite{Saha2007} states that there is no noticeable difference between
the speedup of work-stealing and global shared work queue when using
not more than 8 cores.

To check this hypothesis, we rewrote the intervals scheduler to use a
single shared work deque and use it in a LIFO (stack) manner.

\minisec{Limits of work-stealing scheduling \cite{Vrba2009}}

The original work-stealing algorithm uses non-blocking algorithms to
implement queue operations \cite{Arora2001}. However, we have decided
to simplify our scheduler implementation by protecting each run queue
with its own lock. We believed that this would not impact scalability
on our machine, because others \cite{Saha2007} have reported that even
a single, centralized queue protected by a single, central lock does
not hurt performance on up to 8 CPUs, which is a decidedly worse
situation for scalability as the number of CPUs grows. Since we use
locks to protect the run queues, and our networks are static, our
implementation does not benefit from the first two advantages of
accessing the run queues at different ends:

The reasons for accessing the run queues at different ends are several
\cite{Frigo1998}: 1) it reduces contention by having stealing threads
operate on the opposite end of the queue than the thread they are
stealing from; 2) it works better for parallelized divide-and-conquer
algorithms which typically generate large chunks of work early, so the
older stolen task is likely to further provide more work to the
stealing thread; 3) stealing a process also migrates its future
workload, which helps to increase locality.

Nevertheless, this helps with increasing locality: since the arrival
of a message unblocks a proces, placing it at the front of the ready
queue increases probability that the required data will remain in the
CPUâ€™s caches.

\todo{Mention the experiment with a single shared work deque (stack)}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
