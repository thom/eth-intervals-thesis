%==============================================================================
% queues-description.tex
%==============================================================================

\chapter{Work-Stealing Queues}
\label{chap:queues-description}

\todo{Change chapter title to ``Background''?}

\section{References}

\subsection{The art of multiprocessor programming \cite{Herlihy2008}}

Ideally, a work-stealing algorithm should provide a linearizable
implementation whose take and steal methods always return a task if
one is available. In practice, however, we can settle for something
weaker, allowing a \lstinline!steal()! call to return \lstinline!null!
if it conflicts with a concurrent \lstinline!steal()! call. Though we
could have the unsuccessful thief simply try again, it makes more
sense in this context to have a thread retry the \lstinline!steal()!
operation on a different, randomly chosen deque each time. To support
such a retry, a \lstinline!steal()! call may return \lstinline!null!
if it conflicts with a concurrent \lstinline!steal()! call.

\todo{Rephrase paragraph}

\subsection{Dynamic circular work-stealing deque \cite{Chase2005}}

The ABP work-stealing algorithm of Arora, Blumofe, and Plaxton
\cite{Arora2001} has been gaining popularity as the multiprocessor
load-balancing technology of choice in both industry and academia
\cite{Arora2001, Acar2002, Blumofe1995, Frigo1998, Danaher2005}. The
scheme implements a provably efficient work-stealing paradigm due to
Blumofe and Leiserson \cite{Blumofe1999} that allows each process to
maintain a local work deque\footnote{Actually, the work-stealing
  algorithm uses a work-stealing deque, which is like a deque
  \cite{Knuth1997} except that only one process can access one end of
  the queue (the ``bottom''), and only pop operations can be invoked
  on the other end (the ``top'').  For brevity, we refer to the data
  structure as a deque in the remainder of the paper.} and steal an
item from others if its deque becomes empty. The deque's owner process
puts and takes local work to and from the deque's bottom end. To
minimize synchronization overhead for the deque's owner, elements are
stolen from the top end of the deque. No elements are added to the top
end of the deque. An ABP deque thus presents three methods in its
interface:

\begin{itemize}
\item \lstinline!put(Object o)!: Puts \lstinline!o! onto the bottom
  of the deque.
\item \lstinline!Object take()!: Takes an object from the bottom of the
  deque if the deque is not empty, otherwise returns
  \lstinline!Empty!.
\item \lstinline!Object steal()!: If the deque is empty, returns
  \lstinline!Empty!. Otherwise, returns the element successfully
  stolen from the top of the deque, or returns \lstinline!Abort! if
  this process loses a race with another process to steal the topmost
  element.\footnote{In our implementation, as we describe, Abort is
    also returned if a steal operation lost a race with an array
    memory reclamation caused by a concurrent \lstinline!take!
    operation.}
\end{itemize}

Note that \lstinline!put! and \lstinline!take! operations
are invoked only by the deque's owner.

Unfortunately, the use of fixed size arrays introduces an inefficient
memory-size/robustness tradeoff: for $n$ processes and total allocated
memory size $m$, one can tolerate at most $\frac{m}{n}$ items in a
deque. Using cyclic arrays, or the reset-on- empty heuristic presented
in the original ABP algorithm,\footnote{The reset-on-empty heuristic
  resets top and bottom to point to the beginning of the array
  whenever the deque becomes empty. It was used by the original ABP
  algorithm to make overflow scenarios less frequent.} reduces the
chance of overflow but does not eliminate it.

\subsection{A dynamic-sized nonblocking work stealing deque
  \cite{Hendler2006, Hendler2006a}}

In work-stealing scheduling, each process tries to work on its newly
created threads locally, and attempts to steal threads from other
processes only when it has no local threads to execute. This way, the
computational overhead of re-balancing is paid by the processes that
would otherwise be idle.

The ABP work-stealing algorithm of Arora, Blumofe, and Plaxton
\cite{Arora2001} has been gaining popularity as the multiprocessor
load-balancing technology of choice in both industry and academia
\cite{Arora2001, Acar2002, Blumofe1995, Frigo1998, Danaher2005}. The
scheme implements a provably efficient work-stealing paradigm due to
Blumofe and Leiserson \cite{Blumofe1999} that allows each process to
maintain a local work deque,\footnote{Actually, the work stealing
  algorithm uses a work stealing deque, which is like a deque
  \cite{Knuth1997} except that only one process can access one end of
  the queue (the ``bottom''), and only pop operations can be invoked
  on the other end (the ``top''). For brevity, we refer to the data
  structure as a deque in the remainder of the paper.} and steal an
item from others if its deque becomes empty. It has been extended in
various ways such as stealing multiple items \cite{Hendler2002} and
stealing in a locality-guided way \cite{Acar2002}. At the core of the
ABP algorithm is an efficient scheme for stealing an item in a
non-blocking manner from an array-based deque, minimizing the need for
costly Compare-and-Swap (CAS)\footnote{The CAS (location, old-value,
  new-value) operation atomically reads a value from location, and
  writes new-value in location if and only if the value read is
  old-value. The operation returns a boolean indicating whether it
  succeeded in updating the location.} synchronization operations when
fetching items locally.

Unfortunately, the use of fixed size arrays\footnote{One may use
  cyclic array indexing but this does not help in preventing
  overflows.} introduces an inefficient memory-size/robustness
tradeoff: for $n$ processes and total allocated memory size $m$, one
can tolerate at most $\frac{m}{n}$ items in a deque. Moreover, if
overflow does occur, there is no simple way to malloc additional
memory and continue. This has, for example, forced parallel garbage
collectors using work-stealing to implement an application-specific
blocking overflow management mechanism [5, 10]. In multiprogrammed
systems, the main target of ABP work-stealing \cite{Arora2001}, even
inefficient over-allocation based on an application's maximal
execution-DAG depth \cite{Arora2001, Blumofe1999} may not always
work. If a small subset of non-preempted processes end up queuing most
of the work items, since the ABP algorithm sometimes starts putting
items from the middle of the array even when the deque is empty, this
can lead to overflow.\footnote{The ABP algorithm's built-in ``reset on
  empty'' mechanism helps in some, but not all, of these cases.}

This state of affairs leaves open the question of designing a dynamic
memory algorithm to overcome the above drawbacks, but to do so while
maintaining the low-cost synchronization overhead of the ABP
algorithm. This is not a straightforward task, since the the
array-based ABP algorithm is unique: it is possibly the only
real-world algorithm that allows one to transition in a lock-free
manner from the common case of using loads and stores to using a
costly CAS only when a potential conflict requires processes to
synchronize. This transition rests on the ability to detect these
boundary synchronization cases based on the relative gap among array
indexes. There is no straightforward way of translating this
algorithmic trick to the pointer-based world of dynamic data
structures.

\subsection{A Java fork/join framework \cite{Lea2000}}

\subsubsection{Deques}

To enable efficient and scalable execution, task management must be
made as fast as possible. Creating, putting, and later taking (or,
much less frequently, stealing) tasks are analogs of procedure call
overhead in sequential programs. Lower overhead enables programmers to
adopt smaller task granularities, and in turn better exploit
parallelism.

Task allocation itself is the responsibility of the JVM. Java garbage
collection relieves us of needing to create a special-purpose memory
allocator to maintain tasks. This substantially reduces the complexity
and lines of code needed to implement intervals compared to similar
frameworks in other languages. The basic structure of the deque
employs the common scheme of using a single (although resizable) array
per deque, along with two indices: The top index acts just like an
array-based stack pointer, changing upon put and take. The bottom
index is modified only by steal.

Because the deque array is accessed by multiple threads, sometimes
without full synchronization, yet individual Java array elements
cannot be declared as volatile, each array element is actually a fixed
reference to a little forwarding object maintaining a single volatile
reference. This decision was made originally to ensure conformance
with Java memory rules, but the level of indirection that it entails
turns out to improve performance on tested platforms, presumably by
reducing cache contention due to accesses of nearby elements, which
are spread out a bit more in memory due to the indirection.

The main challenges in deque implementation surround synchronization
and its avoidance. Even on JVMs with optimized synchronization
facilities, the need to obtain locks for every put and take operation
becomes a bottleneck.  However, adaptations of tactics taken in Cilk
\cite{Frigo1998} provide a solution based on the following
observations:

\begin{itemize}
\item The put and take operations are only invoked by owner threads.
\item Access to the steal operation can easily be confined to one
  stealing thread at a time via an entry lock on steal. This deque
  lock can also serve to disable steal operations when
  necessary. Thus, interference control is reduced to a two-party
  synchronization problem.
\item The take and steal operations can only interfere if the deque is
  about to become empty. Otherwise they are guaranteed to operate on
  disjoint elements of the array.
\end{itemize}

Defining the top and bottom indices as volatile ensures that a take
and steal can proceed without locking if the deque is sure to have
more than one element. This is done via a Dekker-like algorithm in
which put pre-decrements top:

\begin{lstlisting}
if (--top >= bottom) ...
\end{lstlisting}

and steal pre-increments bottom:

\begin{lstlisting}
if (++bottom < top) ...
\end{lstlisting}

In each case they must then check to see if this could have caused the
deque to become empty by comparing the two indices. An asymmetric rule
is used upon potential conflict: take rechecks state and tries to
continue after obtaining the deque lock (the same one as held by
steal), backing off only if the deque is truly empty. A steal
operation instead just backs off immediately, typically then trying to
steal from a different victim. This asymmetry represents the only
significant departure from the otherwise similar THE protocol used in
Cilk.

The use of volatile indices also enables the put operation to proceed
without synchronization unless the deque array is about to overflow,
in which case it must first obtain the deque lock to resize the
array. Otherwise, simply ensuring that top is updated only after the
deque array slot is filled in suppresses interference by any steal.

\subsubsection{Stealing and Idling}

Worker threads in work-stealing frameworks know nothing about the
synchronization demands of the programs they are running. They simply
generate, put, take, steal, manage the status of, and execute
tasks. The simplicity of this scheme leads to efficient execution when
there is plenty of work for all threads. However, this streamlining
comes at the price of relying on heuristics when there is not enough
work; i.e., during startup of a main task, upon its completion, and
around global full-stop synchronization points employed in some
fork/join algorithms.

The main issue here is what to do when a worker thread has no local
tasks and cannot steal one from any other thread. If the program is
running on a dedicated multiprocessor, then one could make the case
for relying on hard busy-wait spins looping to try to steal
work. However, even here, attempted steals increase contention, which
can slow down even those threads that are not idle due to locking
protocols.  Additionally, in more typical usage contexts of this
framework, the operating system should somehow be convinced to try to
run other unrelated runnable processes or threads.

The tools for achieving this in Java are weak, have no guarantees
\cite{Goetz2006}, but usually appear to be acceptable in practice (as
do similar techniques described for Hood \cite{Blumofe1998}). A thread
that fails to obtain work from any other thread lowers its priority
before attempting additional steals, performs \lstinline!Thread.yield!
between attempts, and registers itself as inactive in its thread
pool. If all others become inactive, they all block waiting for
additional main tasks. Otherwise, after a given number of additional
spins, threads enter a sleeping phase, where they sleep (for up to
100ms) rather than yield between steal attempts. These imposed sleeps
can cause artificial lags in programs that take a long time to split
their tasks. But this appears to be the best general-purpose
compromise. Future versions of the framework may supply additional
control methods so that programmers can override defaults when they
impact performance.

\section{Interface}
\label{sec:queues-description-interface}

\lstinputlisting[style=Float,
  caption={Work-stealing queue interface}, 
  label=lst:work-stealing-queue-interface]{
    ../listings/queues-description/WorkStealingQueue.java
}

\todo{Finish section ``Interface''}

\section{Current Queue Implementation}
\label{sec:queues-description-current-implementation}

This is the deque currently used by the intervals scheduler. It is
called ``Lazy'' because the owner of the deque only lazily updates the
location of the head of deque, i.e., only when it tries to take
something and finds it gone.

\todo{Finish section ``LazyDeque''}

The members of the queue are defined as (see paper about duplicating
queues):

\lstinputlisting[style=Numbers,
  caption={Lazy deque}, 
  label=lst:work-stealing-lazy-deque]{
    ../listings/queues-description/WorkStealingLazyDeque.java
}

\lstinputlisting[style=Numbers,
  caption={Lazy deque: Put}, 
  label=lst:work-stealing-lazy-deque-put]{
    ../listings/queues-description/WorkStealingLazyDeque-put.java
}

\lstinputlisting[style=Numbers,
  caption={Lazy deque: Take}, 
  label=lst:work-stealing-lazy-deque-take]{
    ../listings/queues-description/WorkStealingLazyDeque-take.java
}

\lstinputlisting[style=Numbers,
  caption={Lazy deque: Steal}, 
  label=lst:work-stealing-lazy-deque-steal]{
    ../listings/queues-description/WorkStealingLazyDeque-steal.java
}

\lstinputlisting[style=Numbers,
  caption={Lazy deque: Expand}, 
  label=lst:work-stealing-lazy-deque-expand]{
    ../listings/queues-description/WorkStealingLazyDeque-expand.java
}

% \begin{center}
%   \begin{tikzpicture}
%     % \node[text centered,text width=4cm]{Description};
    
%     \begin{scope}[line width=4mm,rotate=270]
%       \newcount\mycount
%       \foreach \angle in {0,360,...,3599}
%       {
%         \mycount=\angle\relax
%         \divide\mycount by 10\relax
%         \draw[black!15,thick] (\the\mycount:2.5cm) -- (\the\mycount:4cm);
%       }
      
%       \draw (162:4.2cm) node[above] {0};
%       \draw (126:4.2cm) node[right] {1};
%       \draw (90:4.2cm) node[right] (topindex) {2};
%       \draw (54:4.2cm) node[below] {3};
%       \draw (18:4.2cm) node[below] {4};
%       \draw (342:4.2cm) node[below] {5};    
%       \draw (306:4.2cm) node[below] {6};
%       \draw (270:4.2cm) node[left] {7};
%       \draw (234:4.2cm) node[left] {8};
%       \draw (198:4.2cm) node[above] {9};
%     \end{scope}  
    
%     \filldraw[gray] (3.25cm,0cm) circle (0.4cm);
    
%     \draw (7cm,0cm) node (topbox) {top};
%     \draw[->] (topbox) -- (topindex);
    
%     \draw[gray] (0,0) circle (4cm) circle (2.5cm);
%   \end{tikzpicture}
% \end{center}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
