%==============================================================================
% locality-implementation.tex
%==============================================================================

\chapter{Implementation}
\label{chap:locality-implementation}

In this chapter we describe our new implementation of the intervals
scheduler. It is designed for locality-aware scheduling using locality
hints provided by the programmer. Instead of using work-stealing
workers our scheduler design uses \emph{Work-Stealing Places}. 

The API to use locality-aware intervals is introduced in Section
\ref{sec:locality-implementation-locality-aware-intervals-api}. Section
\ref{sec:locality-implementation-work-stealing-places} presents the
idea and implementation of work-stealing places. Each worker is
implemented as a separate Java thread. For the locality-aware
scheduler we need to bind every worker thread to a separate core. In
Section \ref{sec:locality-implementation-core-affinity} we show how we
can set the core affinity of Java threads.


\section{Locality-Aware Intervals API}
\label{sec:locality-implementation-locality-aware-intervals-api}

Intervals are represented as subtypes of the abstract class
\lstinline!Interval!. To make an interval locality-aware, the
programmer has to specify the interval's locality when creating it. We
extend the abstract \lstinline!Interval! class to support locality
hints:

\lstinputlisting[style=Skip, 
  caption={Locality-aware \lstinline!Interval! class},
  label=lst:locality-implementation-interval]{
    ../listings/locality-implementation/Interval.java
}

Locality hints are provided in the form of instances of the
\lstinline!PlaceID! class. This specifies which
place the interval should be executed on. When \lstinline!null! is
specified, the scheduler assigns the interval to a place in a
round-robin fashion.

The \lstinline!PlaceID! class (Listing
\ref{lst:locality-implementation-place-id}) encapsulates an integer ID
and name. We could also use an integer for places directly, but
decided to use a class due to type safety.

\lstinputlisting[style=Float, 
  caption={\lstinline!PlaceID! class},
  label=lst:locality-implementation-place-id]{
    ../listings/locality-implementation/PlaceID.java
}

Note that the \lstinline!PlaceID! is an abstract class since we do not
want the programmers to create their own places. To get a
\lstinline!PlaceID!  instance, the programmer has to use the
\lstinline!Config! class (Listing
\ref{lst:locality-implementation-config}).

\lstinputlisting[style=Float, 
  caption={\lstinline!Config! class},
  label=lst:locality-implementation-config]{
    ../listings/locality-implementation/Config.java
}

We introduce the \lstinline!Config! class to simplify switching to
another machine as the current scheduler implementation cannot
discover places automatically. \lstinline!Config! features an instance
of the \lstinline!Places! class which we describe in Section
\ref{sec:locality-implementation-work-stealing-places}. Programmers
can use it to get places,
e.g. \lstinline!Config.places.getPlaceID(0)!.


\section{Work-Stealing Places}
\label{sec:locality-implementation-work-stealing-places}

A work-stealing scheduler employs a fixed number of threads called
workers. In traditional work-stealing scheduler designs, each worker
has a local double-ended queue, or deque, to maintain its own pool of
ready tasks from which it obtains work. We use so-called
\emph{Work-Stealing Places} instead. Each work-stealing place has a
fixed number of workers and a local deque to maintain ready tasks. The
workers of a place share its local deque from which they obtain
work. When a worker finds that its place's pool is empty, it becomes a
thief and steals a task from the pool of a victim place chosen at
random.

Figure \ref{fig:locality-implementation-work-stealing-places} shows
the work-stealing places used in our Intel Nehalem testing machine
(Section \ref{sec:experimental-setup-mafushi}).

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{locality-implementation/places}
  \caption{Work-Stealing Places}
  \label{fig:locality-implementation-work-stealing-places}
\end{figure}

Other locality-aware work-stealing schedulers use a more complex
design. \textcite{Acar2002} for example provide each worker in
addition to a deque with a mailbox. A mailbox is a FIFO queue of
pointers to intervals that have affinity for the worker. So when
creating an interval, a worker will push it onto both the deque, as in
normal work-stealing, and also onto the tail of the mailbox of the
worker that the interval has affinity for. Now a worker will first try
to obtain work from its mailbox before attempting to steal. Because
intervals can appear twice, once in a mailbox and once on a deque,
they have to be idempotent (Section
\ref{sec:queues-implementation-idempotent-ws-deque}).

However, we have decided to simplify our scheduler implementation by
using a shared deque per \emph{Work-Stealing Place}. We believe that
this would not impact scalability as long as the places are not too
large, because we could show in Section
\ref{sec:queues-performance-single-shared-queue} that up to 8 workers
there is no significant difference between using a separate deque for
each worker or a shared deque per place.

\subsection{Places Implementation}
\label{sec:locality-implementation-work-stealing-places-implementation}

\todo{Finish section}

\lstinputlisting[style=FloatNumbers, 
  caption={TODO},
  label=lst:locality-implementation-places]{
    ../listings/locality-implementation/Places.java
}

\lstinputlisting[style=FloatNumbers, 
  caption={TODO},
  label=lst:locality-implementation-marvin-places]{
    ../listings/locality-implementation/MarvinPlaces.java
}

\lstinputlisting[style=FloatNumbers, 
  caption={TODO},
  label=lst:locality-implementation-mafushi-places]{
    ../listings/locality-implementation/MafushiPlaces.java
}

\subsection{Scheduling}
\label{sec:locality-implementation-work-stealing-places-scheduling}

\todo{Finish section}

\begin{lstlisting}[style=FloatNumbers,
  caption={TODO},
  label=lst:locality-implementation-submit]
void submit(WorkItem item) {
  Worker worker;
  PlaceID placeID = item.getPlaceID();
    
  if (placeID == null) {
    worker = currentWorker.get();
    if (worker != null) {
      worker.place.enqueue(item);
    }
    else {
      // Round robin assignment
      places[nextPlace].enqueue(item);
      nextPlace = (nextPlace + 1) % numberOfPlaces;
    }
  } else {
    places[placeID.id].enqueue(item);
  }
}
\end{lstlisting}

\todo{Finish section ``Work-Stealing Places''}


\section{Setting Core Affinity of Worker Threads}
\label{sec:locality-implementation-core-affinity}

In recent Java Virtual Machines, threads are implemented with native
threads, so a Java program using threads is no different from a native
program using threads: A Java thread is just a thread belonging to a
JVM process. This means there is a 1-to-1 correspondence between Java
and native threads. When using the GNU C library on Linux, native
threads are implemented with the NPTL (Native POSIX Threads
Library). NPTL is also an 1-to-1 implementation, meaning that each
thread maps to a kernel scheduling entity (Figure
\ref{fig:locality-implementation-core-affinity-thread-mapping}).

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}
    [node distance=0.8cm,
    start chain=going below,]
    \node[box, join] {Java Thread};
    \node[box, join] {Native POSIX Thread};
    \node[box, join] {Kernel Scheduling Entity};
  \end{tikzpicture}
  \caption{Linux 1-to-1 thread mapping}
  \label{fig:locality-implementation-core-affinity-thread-mapping}
\end{figure}

Unfortunately the Java Threads API does not expose the ability to set
the CPU or core affinity despite numerous use cases where setting the
affinity of threads would be beneficial -- such as improving cache and
network performance or real-time applications \cite{Love2003, Dow2005,
  Foong2008}. There exists a request for enhancement on this issue but
it got set to the state \emph{``Closed, Will Not Fix''}
\cite{Oracle1999}.

\subsection{JNI Library}
\label{sec:locality-implementation-core-affinity-jni-library}

To bind the workers to a specific core, we wrote a small JNI library
-- see Listing \ref{lst:locality-implementation-core-affinity} for the
API. The method \lstinline!set(int physicalUnit)!  (Line
\ref{lst:locality-implementation-core-affinity-set-unit}) is used to
bind the current thread to a physical unit. With
\lstinline!set(int[] physicalUnits)! the current thread can be bound
to several physical units, for example to a node in a NUMA system
(Line \ref{lst:locality-implementation-core-affinity-set-node}). For
debugging purposes we also implemented the method \lstinline!get()!
which returns a boolean array whose elements are true if the current
thread has an affinity set to the corresponding physical unit (Line
\ref{lst:locality-implementation-core-affinity-get}).

\lstinputlisting[style=FloatNumbers,
  caption={\lstinline{Affinity} class with interface for the native methods}, 
  label=lst:locality-implementation-core-affinity]{
    ../listings/locality-implementation/Affinity.java 
}

Listing \ref{lst:locality-implementation-core-affinity-jni-header}
contains the native method declarations which are implemented in
Listing \ref{lst:locality-implementation-core-affinity-jni-set} and
\ref{lst:locality-implementation-core-affinity-jni-get}. Line
\ref{lst:locality-implementation-core-affinity-jni-h-set-unit}
declares \lstinline!set(int physicalUnit)!. On Line
\ref{lst:locality-implementation-core-affinity-jni-h-set-node} we
declare the method \lstinline!set(int[] physicalUnits)! and
\lstinline!get()! is declared on Line
\ref{lst:locality-implementation-core-affinity-jni-h-get}.

\lstinputlisting[style=FloatNumbersC, 
  caption={\lstinline{Affinity} class: JNI C header}, 
  label=lst:locality-implementation-core-affinity-jni-header]{
    ../listings/locality-implementation/ch_ethz_hwloc_Affinity.h 
}

\lstinputlisting[style=FloatNumbersC, 
  caption={\lstinline{Affinity} class: JNI C implementation to set the affinity},
  label=lst:locality-implementation-core-affinity-jni-set]{
    ../listings/locality-implementation/ch_ethz_hwloc_Affinity_set.c 
}

The implementation for setting the affinity is shown in Listing
\ref{lst:locality-implementation-core-affinity-jni-set}. When setting
the affinity, we first initialize the CPU set structure (Lines
\ref{lst:locality-implementation-core-affinity-jni-set-unit-init-cpuset}
and
\ref{lst:locality-implementation-core-affinity-jni-set-node-init-cpuset})
and then we set it to the specified physical units (Lines
\ref{lst:locality-implementation-core-affinity-jni-set-unit-set-cpuset}
and
\ref{lst:locality-implementation-core-affinity-jni-set-node-set-cpuset}). The
actual setting of the affinity is done in function
\lstinline!set_affinity()! on Lines
\ref{lst:locality-implementation-core-affinity-jni-set-impl-start} --
\ref{lst:locality-implementation-core-affinity-jni-set-impl-end}.

To get the thread ID, we use the \lstinline!pthread_self()! function
(line
\ref{lst:locality-implementation-core-affinity-jni-set-impl-pthread-self}). The
function \lstinline!pthread_setaffinity_np()! (Line
\ref{lst:locality-implementation-core-affinity-jni-set-impl-set}) sets
the CPU affinity mask of the thread to the CPU set pointed to by
\lstinline!cpuset!.  If the call is successful, and the thread is not
currently running on one of the CPUs in \lstinline!cpuset!, then it is
migrated to one of those CPUs. If there is an error,
\lstinline!pthread_setaffinity_np()!  returs a nonzero error number
and we throw an exception (Line
\ref{lst:locality-implementation-core-affinity-jni-set-impl-set-error}).

Listing \ref{lst:locality-implementation-core-affinity-jni-get} shows
the implementation for getting the affinity of a thread. This method
is mainly used for debugging purposes. We use
\lstinline!pthread_self()! (Line
\ref{lst:locality-implementation-core-affinity-jni-get-impl-pthread-self})
to get the thread ID and then call function
\lstinline!pthread_getaffinity_np()!  (Line
\ref{lst:locality-implementation-core-affinity-jni-get-impl-get}). The
\lstinline!pthread_getaffinity_np()! function returns the CPU affinity
mask of the thread thread in the buffer pointed to by
\lstinline!cpuset!.

\lstinputlisting[style=FloatNumbersC, 
  caption={\lstinline{Affinity} class: JNI C implementation to get the affinity},
  label=lst:locality-implementation-core-affinity-jni-get]{
    ../listings/locality-implementation/ch_ethz_hwloc_Affinity_get.c 
}

\subsection{Worker}
\label{sec:locality-implementation-core-affinity-worker}

\todo{Finish section}

\begin{lstlisting}[style=FloatNumbers,
  caption={TODO},
  label=lst:locality-implementation-worker]
class Worker extends Thread {
  final int id;
  
  Worker(int id, Place place) {
    super(place.getName() + "-Worker-" + id);
    this.id = id;
  }

  public void run() {
    try {
      Affinity.set(id);
    } catch (SetAffinityException e) {
      e.printStackTrace();
    }
    
    currentWorker.set(this);
    
    while (true) {
      doWork(true);
    }
  }

  // ...
}
\end{lstlisting}

\subsection{Restrictions}
\label{sec:locality-implementation-core-affinity-restrictions}

\subsubsection{Portability}

As we are directly using functions provided by POSIX threads, our
implementation is not portable across operating systems that do not
support the POSIX standard. To make our library portable, it could be
rewritten using \emph{Portable Linux Processor Affinity (PLPA)}
\cite{OpenMPI2010a} or \emph{Portable Hardware Locality (hwloc)}
\cite{OpenMPI2010}.

\subsubsection{Data Locality}

By setting the core affinity of threads, we only control the locality
of the work but we do not have control over data locality.

On a NUMA system every processor in the system has a local memory that
provides low access latency and high bandwidth, and a remote memory
that is considerably slower to access.

In the Java HotSpot VM, the NUMA-aware allocator has been implemented
to provide automatic memory placement optimisations for Java
applications \cite{Masamitsu2008, Oracle2010, Humble2010}: The
allocator controls the eden space of the young generation of the heap,
where most of the new objects are created. It divides the space into
regions each of which is placed in the memory of a specific node. The
allocator relies on a hypothesis that a thread that allocates the
object will be the most likely to use the object. To ensure the
fastest access to the new object, the allocator places it in the
region local to the allocating thread.

On Linux, the implementation is based on \cite{Kleen2004}. It can be
enabled by using the option \verb!-XX:+UseNUMA!.

\subsubsection{Contention With Other Threads}

The affinity of the workers is set such that they execute on different
cores. While this eliminates interference between the worker threads,
they will nevertheless share their assigned core with other processes
in the system, subject to standard Linux scheduling policy.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
