%==============================================================================
% locality-introduction.tex
%==============================================================================

\part{Locality-Aware Work-Stealing}
\label{part:locality}

\chapter{Introduction}
\label{chap:locality-introduction}

In a shared-memory multiprocessor system, it may be more efficient to
schedule a task on one processor than another. One reason for this is
that the processors, or their associated resources, may be
heterogeneous; we consider this case in more detail below. Another
reason is processor-cache affinity: when a task returns for execution
and is scheduled on a processor, it experiences an initial burst of
cache misses. However, if a significant portion of the task's working
set is already in the cache, this penalty is reduced. Thus we have a
tradeoff between load balancing (assigning tasks to less loaded
processors) and using locality (assigning tasks to processors with
high affinity).

\begin{table}[htb]
  \centering
  \begin{tabular}{ll}
    \toprule
    Data Source & Latency \\\midrule
    L3 cache hit, line unshared & $\sim 40$ cycles\\
    L3 cache hit, shared line in another core\hspace{0.5cm} & $\sim 65$ cycles \\
    L3 cache hit, modified in another core & $\sim 75$ cycles \\
    Remote L3 cache & $\sim 100 - 300$ cycles \\
    Local DRAM & $\sim 60$ ns \\
    Remote DRAM & $\sim 100$ ns \\\bottomrule
  \end{tabular}
  \caption[Memory access times on the Intel Nehalem processor]
  {Memory access times on the Intel Nehalem processor}
  \label{tab:locality-introduction-memory-access-times}
\end{table}

Table
\ref{tab:locality-introduction-memory-access-times}\footnote{\cite{Levinthal2009}:
  These values are rough approximations. They depend on the core and
  uncore frequencies, memory speeds, BIOS settings, etc.}

\section{Problem and Motivation}
\label{sec:locality-intro-problem-and-motivation}

\todo{Finish section ``Problem and Motivation''}

\subsection*{References}

\subsubsection{Caches}

The speed of processors has grown to be much faster than main
memory. Making all of memory nearly as fast as a processor would
simply prove too expensive for most computers. Instead, designers make
small amounts of memory, known as caches, operate nearly as fast as
the processor. The main memory can then be slower and more
affordable. The hardware knows how to move information in and out of
caches as needed, thereby adding to the number of places where data is
shuffled on its journey between memory and the processor cores. Caches
are critical in helping overcome the mismatch between memory speed and
processor speed.

Virtually all computers use caches only for a temporary copy of data
that should eventually reside in memory. Therefore, the function of a
memory subsystem is to move data needed as input by each processing
core to caches near that processor core, and to move data produced by
the processing cores out to main memory. As data is read from memory
into the caches, some data needs to be evicted from the cache. Cache
designers work to make the data evicted be approximately the data
least likely to be used again.

Once a processor accesses data, it is best to exhaust the programâ€™s
use of it while it is still in the cache. Continued usage will hold it
in the cache, whereas prolonged inactivity will likely lead to its
eviction and future usage will need to do a more expensive (slow)
access to get the data. Furthermore, every time an additional thread
runs on a processor core, data is likely to be discarded from the
cache.

A subtler cost is cache cooling. Processors keep recently accessed
data in cache memory, which is very fast, but also relatively small
compared to main memory. When the processor runs out of cache memory,
it has to evict items from cache and put them back into main
memory. Typically, it chooses the least recently-used items in the
cache. (The reality of set-associative caches is a bit more
complicated, but this is not a cache primer.)

When a logical thread gets its time slice, as it references a piece of
data for the first time, this data is pulled into cache, taking
hundreds of cycles. If it is referenced frequently enough not to be
evicted, each subsequent reference will find it in cache, and take
only a few cycles. Such data is called hot in cache.

Time slicing undoes this because if Thread A finishes its time slice,
and subsequently Thread B runs on the same physical thread, B will
tend to evict data that was hot in cache for A, unless both threads
need the data. When Thread A gets its next time slice, it will need to
reload evicted data, at the cost of hundreds of cycles for each cache
miss. Or worse yet, the next time slice for Thread A may be on a
different physical thread that has a different cache altogether.

\subsubsection{Brief announcement: parallel depth first vs. work
  stealing schedulers on CMP architectures \cite{Liaskovitis2006}}

In the work-stealing scheduler the interval implementation is using,
each processing core maintains a local work double-ended queue (deque)
of ready-to-execute intervals. Whenever its local deque is empty, the
core steals an interval from the bottom of the first non-empty deque
it finds. Work-stealing is an attractive scheduling policy because
when there is plenty of parallelism, stealing is quite rare. However,
work-stealing is not designed for constructive cache sharing, because
the cores tend to have disjoint working sets.

\subsubsection{Scheduling threads for constructive cache sharing on
  CMPs \cite{Chen2007}}

Work Stealing (WS) is a popular greedy thread scheduling
algorithm\footnote{In a greedy schedule, a ready job remains
  unscheduled only if all processors are already busy.} for programs,
with proven theoretical properties with regards to memory and cache
usage \cite{Blumofe1998, Blumofe1999, Acar2000}. The policy maintains
a work queue for each processor (actually a double-ended queue which
allows elements to be inserted on one end of the queue, the top, but
taken from either end). When forking a new thread, this new thread is
placed on the top of the local queue. When a thread completes on a
processor, the processor looks for a ready-to-execute thread by first
looking on the top of the local queue. If it finds a thread, it takes
the thread off the queue and runs it. If the local queue is empty it
checks the work queues of the other processors and steals a thread
from the bottom of the first non-empty queue it finds. WS is an
attractive scheduling policy because when there is plenty of
parallelism, stealing is quite rare and, because the threads in a
queue are related, there is good affinity among the threads executed
by any one processor. However, WS is not designed for constructive
cache sharing, because the processors tend to have disjoint working
sets.

\subsubsection{Trivendi}

Non-uniformity: Apart from the much discussed non uniform memory
access (NUMA) architectures, multiple level of cache sharing is also
common in manycore processors. Hence a flat uniform shared memory
model may not be the best way to model this complex memory hierarchy.
Additionally the actual cost of accessing memory depends depends upon
interconnect topologies like Hypertransport and Quickpath, and factors
such as hop count and routing play an important role.

\subsubsection{Multicore Systems}

\begin{itemize}
\item[\textbullet] Analyzing and Resolving multi-core non scaling on
  Intel Core 2 processors \cite{Levinthal2007}
\item[\textbullet] Memory-Conscious Scheduling for Multicore Systems
  \cite{Majo2010}
\item[\textbullet] Efficient Data Sharing in Intel
  \textsuperscript{\textregistered} Core Microarchitecture Based
  Systems \cite{Shemer2007}
\item[\textbullet] Scheduling threads for constructive cache sharing
  on CMPs \cite{Chen2007}
\end{itemize}

\subsubsection{Memory}
\label{sec:lr-memory}

\begin{itemize}
\item[\textbullet] What every programmer should know about memory
  \cite{Drepper2007}
\item[\textbullet] Understanding Application Memory Performance
  \cite{Drepper2008}
\end{itemize}

\section{Aim}
\label{sec:locality-intro-aim}

\todo{Finish section ``Aim''}

\section{Overview}
\label{sec:locality-intro-overview}

Chapter \ref{chap:locality-approach} describes our approach in
evaluating locality-aware scheduling. Chapter
\ref{chap:locality-implementation} explains the locality-aware
implementation of the intervals scheduler which uses locality hints
provided by the programmer. The chapter presents the locality-aware
intervals API, introduces \emph{Work-Stealing Places}, and shows how
worker threads are bound to specific processing units, e.g. cores.  In
Chapter \ref{chap:locality-performance} we describe the locality
benchmarks and analyze their results. Chapter
\ref{chap:locality-related-work} puts our research in the context of
related work. In Chapter \ref{chap:locality-conclusions} we conclude
and summarize our research, and give some ideas for future work.

\todo{Finish chapter ``Introduction''}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
