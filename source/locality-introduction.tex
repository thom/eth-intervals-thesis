%==============================================================================
% locality-introduction.tex
%==============================================================================

\part{Locality-Aware Work-Stealing}
\label{part:locality}

\chapter{Introduction}
\label{chap:locality-introduction}

\section{Problem and Motivation}
\label{sec:locality-intro-problem-and-motivation}

\todo{Finish section ``Problem and Motivation''}

\subsection{References}

\subsubsection{Brief announcement: parallel depth first vs. work
  stealing schedulers on CMP architectures \cite{Liaskovitis2006}}

In the work-stealing scheduler the interval implementation is using,
each processing core maintains a local work double-ended queue (deque)
of ready-to-execute intervals. Whenever its local deque is empty, the
core steals an interval from the bottom of the first non-empty deque
it finds. Work-stealing is an attractive scheduling policy because
when there is plenty of parallelism, stealing is quite rare. However,
work-stealing is not designed for constructive cache sharing, because
the cores tend to have disjoint working sets.

\subsubsection{Scheduling threads for constructive cache sharing on
  CMPs \cite{Chen2007}}

Work Stealing (WS) is a popular greedy thread scheduling
algorithm\footnote{In a greedy schedule, a ready job remains
  unscheduled only if all processors are already busy.} for programs,
with proven theoretical properties with regards to memory and cache
usage \cite{Blumofe1998a, Blumofe1999, Acar2002}. The policy maintains
a work queue for each processor (actually a double-ended queue which
allows elements to be inserted on one end of the queue, the top, but
taken from either end). When forking a new thread, this new thread is
placed on the top of the local queue. When a thread completes on a
processor, the processor looks for a ready-to-execute thread by first
looking on the top of the local queue. If it finds a thread, it takes
the thread off the queue and runs it. If the local queue is empty it
checks the work queues of the other processors and steals a thread
from the bottom of the first non-empty queue it finds. WS is an
attractive scheduling policy because when there is plenty of
parallelism, stealing is quite rare and, because the threads in a
queue are related, there is good affinity among the threads executed
by any one processor. However, WS is not designed for constructive
cache sharing, because the processors tend to have disjoint working
sets.

\subsubsection{Work stealing: an annotated bibliography
  \cite{Neill2001}}

Affinity scheduling: \cite{Squillante1993}, \cite{Squillante2001},
\cite{Acar2002}

In a shared-memory multiprocessor system, it may be more efficient to
schedule a task on one processor than another. One reason for this is
that the processors, or their associated resources, may be
heterogeneous; we consider this case in more detail below. Another
reason is processor-cache affinity: when a task returns for execution
and is scheduled on a processor, it experiences an initial burst of
cache misses. However, if a significant portion of the task's working
set is already in the cache, this penalty is reduced. Thus we have a
tradeoff between load balancing (assigning tasks to less loaded
processors) and using locality (assigning tasks to processors with
high affinity).

\cite{Squillante1993} proposes and compares various scheduling
algorithms which trade off load balancing and processor-cache
affinity. They use the detailed cache model to examine cache reload
time, as well as examining the effects of increased bus
traffic. Analysis techniques: a combination of mean value analysis
(for fixed-processor scheduling), bounding approximations, and
simulation. A shared pool of tasks is assumed: idle processors search
this pool and choose a task associated with that processor if
possible. This outperforms a simple FIFO queue (good load balancing,
bad locality) or a ``fixed processor'' model where each processor has
its own queue and tasks are not shared (good locality, bad load
balancing). No work stealing since pool of tasks is shared.

\cite{Squillante2001} assumes a generic form of affinity in which the
service rates are higher for jobs in a processor's own queue.

\cite{Acar2002} presents a work-stealing algorithm that uses locality
information, and thus outperforms the standard work-stealing algorithm
on benchmarks. Each process maintains a queue of pointers to threads
that have affinity for it, and attempts to steal these first. They
also bound the number of cache misses for the work stealing algorithm,
using a ``potential function'' argument.

\subsubsection{Trivendi}

Non-uniformity: Apart from the much discussed non uniform memory
access (NUMA) architectures, multiple level of cache sharing is also
common in manycore processors. Hence a flat uniform shared memory
model may not be the best way to model this complex memory hierarchy.
Additionally the actual cost of accessing memory depends depends upon
interconnect topologies like Hypertransport and Quickpath, and factors
such as hop count and routing play an important role.

\section{Aim}
\label{sec:locality-intro-aim}

\todo{Finish section ``Aim''}

\section{Overview}
\label{sec:locality-intro-overview}

Chapter \ref{chap:queues-background} summarizes the properties of
work-stealing queues and introduces the \emph{Work-Stealing Lazy
  Deque}, the queue currently used by the intervals scheduler. Chapter
\ref{chap:queues-implementation} describes the investigated queue
implementations. None of the approaches we developed as part of this
research yielded a queue that was improving work-stealing performance
on the machines we had to test them with (Appendix
\ref{sec:experimental-setup-marvin} and
\ref{sec:experimental-setup-mafushi}). Possible reasons for this are
given in the performance evaluation in Chapter
\ref{chap:queues-performance}. Chapter
\ref{chap:queues-conclusions-and-future-work} concludes and summarizes
our findings and encountered problems in order to preserve this
research for future reference.

\todo{Finish section ``Overview''}

\todo{Finish chapter ``Introduction''}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
